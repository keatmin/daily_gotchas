# 27-08-2021
Present my solution of reading/writing `.csv` and `.parquet` between python dataframe and s3, 
WITHOUT using external library like `s3fs` or `aws-data-wrangler`,
and WITHOUT downloading the file.

## CSV
```python
import io

import boto3
import pandas as pd

from typing import Union

def write_csv_to_s3(df: pd.DataFrame, s3_bucket: str, s3_key: str) -> None:
    s3_client = boto3.client(
        "s3",
    )

    with io.StringIO() as csv_buffer:
        df.to_csv(csv_buffer, index=False)

        response = s3_client.put_object(
            Bucket=s3_bucket,
            Key=s3_key,
            Body=csv_buffer.getvalue(),
        )

        status = response.get("ResponseMetadata", {}).get("HTTPStatusCode")

        if status == 200:
            print(
                f"Successful S3 put_object response. {s3_bucket=}, {s3_key=}, {status=}"
            )
        else:
            print(
                f"Unsuccessful S3 put_object response."
                f"{s3_bucket=}, {s3_key=}, {status=}"
            )
          
def read_csv_from_s3(s3_bucket: str, s3_key: str) -> Union[pd.DataFrame, None]:
    s3_client = boto3.client("s3")
    response = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)
    status = response.get("ResponseMetadata", {}).get("HTTPStatusCode")

    if status == 200:
        print(f"Successful S3 get_object response. {s3_bucket=}, {s3_key=}, {status=}")
        df = pd.read_csv(response.get("Body"), lineterminator="\n")
        return df

    print(f"Unsuccessful S3 get_object response. {s3_bucket=}, {s3_key=}, {status=}")
    return None
 ```
 
 ## Parquet
 ```python
import io
from typing import Union

import boto3
import pandas as pd


def write_parquet_to_s3(df: pd.DataFrame, s3_bucket: str, s3_key: str) -> None:
    s3_client = boto3.client(
        "s3",
    )

    with io.BytesIO() as parquet_buffer:
        df.to_parquet(parquet_buffer)

        response = s3_client.put_object(
            Bucket=s3_bucket,
            Key=s3_key,
            Body=parquet_buffer.getvalue(),
        )

        status = response.get("ResponseMetadata", {}).get("HTTPStatusCode")
        if status == 200:
            print(
                f"Successful S3 put_object response. {s3_bucket=}, {s3_key=}, {status=}"
            )
        else:
            print(
                f"Unsuccessful S3 put_object response."
                f"{s3_bucket=}, {s3_key=}, {status=}"
            )


def read_parquet_from_s3(s3_bucket: str, s3_key: str) -> Union[pd.DataFrame, None]:
    s3_client = boto3.client("s3")
    response = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)
    status = response.get("ResponseMetadata", {}).get("HTTPStatusCode")

    if status == 200:
        print(f"Successful S3 get_object response. {s3_bucket=}, {s3_key=}, {status=}")
        df = pd.read_parquet(io.BytesIO(response.get("Body").read()))
        return df

    print(f"Unsuccessful S3 get_object response. {s3_bucket=}, {s3_key=}, {status=}")
    return None
```

### PS
Subtle difference, we can use `StringIO` for csv, but for parquet we need to use `BytesIO`

Why not s3fs? s3fs has dependencies, e.g `aiobotocore` and others that will have conflict with boto3. 
